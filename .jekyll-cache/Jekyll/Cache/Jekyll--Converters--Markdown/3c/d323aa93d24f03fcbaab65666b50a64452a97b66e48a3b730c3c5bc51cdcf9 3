I"l[<h1 id="lecture-07-nlp-深度学习前馈网络">Lecture 07 NLP 深度学习：前馈网络</h1>

<p><strong>什么是深度学习？</strong></p>

<ul>
  <li>机器学习的一个分支。</li>
  <li>它是神经网络的另一个名字。</li>
  <li><strong>神经网络</strong>：灵感来源于大脑的工作机制，包含被称为 <strong>神经元</strong> 的计算单元。</li>
  <li>为什么是 <strong>深度</strong>？因为在现代深度学习模型中，有很多连接在一起的层。</li>
</ul>

<h2 id="1-前馈神经网络">1. 前馈神经网络</h2>

<h3 id="11-总体结构">1.1 总体结构</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-17-WX20200417-204406%402x.png" width="50%" /></p>

<ul>
  <li>又称 <strong>多层感知器（multilayer perceptrons）</strong></li>
  <li>上图是一个包含两个隐藏层的前馈神经网络，图中的每个圆圈表示一个神经元，它实际上对应着某个函数，该函数通常是一个非线性函数（例如：Sigmoid 函数），其输入来自前一层的输出的线性加权组合，而每个神经元的输出的线性组合将作为下一层的输入。以此类推，直到输出层，输出层通常是根据实际问题定制的，例如：处理一个三分类问题时，通常可以将输出层神经元个数设为 3 个。</li>
  <li>图中的每个箭头都携带一个权重，其大小取决于输入变量的重要程度。</li>
  <li>Sigmoid 函数在这里引入了非线性。</li>
</ul>

<h3 id="12-神经元">1.2 神经元</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-17-WX20200417-205859%402x.png" width="40%" /></p>

<p>每个 <strong>神经元</strong> 都是一个 <strong>函数</strong></p>
<ul>
  <li>
    <p>给定输入（向量） $\mathbf x$，计算实值（标量）$h$</p>

\[h=\tanh \left(\sum_{j}w_jx_j+b\right)\]
  </li>
  <li>对输入进行缩放（乘以权重，$\mathbf w$），并且加上偏移量（偏置，$b$）</li>
  <li>采用 <strong>非线性函数</strong>，例如：logistic sigmoid, hyperbolic sigmoid（tanh），或者 ReLU</li>
</ul>

<h3 id="13-矩阵向量表示">1.3 矩阵向量表示</h3>

<ul>
  <li>
    <p>通常会有很多个隐藏神经元，即</p>

\[h_i=\tanh \left(\sum_{j}w_{ij}x_{j}+b_i\right)\]
  </li>
  <li>每个神经元都有属于自己的权重向量 $\mathbf w_i$ 和偏置项 $b_i$</li>
  <li>
    <p>某一层的计算结果可以用矩阵和向量操作表示</p>

\[\mathbf h=\tanh (W\mathbf x + \mathbf b)\]

    <p>其中，$W$ 是一个由权重向量 $\mathbf w_i$ 构成的矩阵，$\mathbf x$ 是输入向量，$\mathbf b$ 是所有偏置项组成的向量。</p>
  </li>
  <li>此时，非线性函数实际上作用在每个元素上。</li>
</ul>

<h3 id="14-输出层">1.4 输出层</h3>

<ul>
  <li>二分类问题（例如：对一条推文进行正面/负面情感分类）
    <ul>
      <li>使用 Sigmoid 激活函数（又称 Logistic 函数）</li>
    </ul>
  </li>
  <li>多分类问题（例如：对一个文档的主题进行分类）
    <ul>
      <li>
        <p>使用 Softmax 函数，确保概率大于 $0$，并且和为 $1$。</p>

\[\left[\dfrac{\exp(v_1)}{\sum_i\exp(v_i)},\dfrac{\exp(v_2)}{\sum_i\exp(v_i)},\dots,\dfrac{\exp(v_m)}{\sum_i\exp(v_i)} \right]\]
      </li>
    </ul>
  </li>
</ul>

<h3 id="15-前馈神经网络">1.5 前馈神经网络</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-17-WX20200417-204406%402x.png" width="50%" /></p>

<p>让我们回到之前的例子，上图是一个包含 1 个输入层、2 个隐藏层和 1 个输出层的前馈神经网络。它可以表示为：</p>

\[\mathbf h_1=\tanh (W_1\mathbf x + \mathbf b_1)\]

\[\mathbf h_2=\tanh (W_2\mathbf h_1 + \mathbf b_2)\]

\[\mathbf y=\text{softmax}(W_3\mathbf h_2)\]

<p>权重矩阵 $W$ 和偏置向量 $\mathbf b$ 就是该模型的所有参数，我们通过定义目标函数，利用梯度下降等方法对其进行训练。</p>

<h3 id="16-从数据中学习">1.6 从数据中学习</h3>

<p><strong>如何从数据中学习参数？</strong></p>

<ul>
  <li>
    <p>本质上，模型试图尽可能地 “拟合” 训练数据，我们可以通过其分配给正确输出的概率来衡量：</p>

\[L=\prod_{i=0}^{m}P(y_i\mid x_i)\]

    <ul>
      <li><strong>最大化</strong> 所有训练数据的总概率 $L$</li>
      <li>等价于 <strong>最小化</strong> $\log L$，对于参数而言</li>
    </ul>
  </li>
  <li>
    <p>训练采用梯度下降</p>
    <ul>
      <li>很多工具，例如 tensorflow、pytorch、dynet 等，利用自动微分来自动计算梯度。</li>
    </ul>
  </li>
</ul>

<h2 id="2-应用">2. 应用</h2>
<p>现在，我们来看一些如何为下游应用构建神经网络的例子。</p>

<h3 id="21-主题分类">2.1 主题分类</h3>
<p>给定一个文档，基于一个预定义的主题集合（例如：经济、政治、体育等）对其进行分类。</p>

<p><strong>输入</strong>：词袋（bag-of-words）</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-18-WX20200418-115639%402x.png" width="70%" /></p>

<p><strong>模型</strong>：
考虑之前的两层模型，假设输出的主题类别一共有 3 类：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-17-WX20200417-204406%402x.png" width="40%" /></p>

\[\mathbf h_1=\tanh (W_1\mathbf x + \mathbf b_1)\]

\[\mathbf h_2=\tanh (W_2\mathbf h_1 + \mathbf b_2)\]

\[\mathbf y=\text{softmax}(W_3\mathbf h_2)\]

<p>对于第一个文档 <strong>doc 1</strong>：<br />
输入向量 $\mathbf x=[0,2,3,0]$ 代表文档的词袋表示<br />
输出向量 $\mathbf y=[0.1,0.6,0.3]$ 代表该文档在 3 个类别上的概率分布</p>

<p><strong>训练过程</strong>：<br />
得到模型输入后，向前传播拟合数据：依次计算第一个隐藏层、第二个隐藏层以及输出层，然后我们可以得到一个输出类别的概率分布，然后我们查看每个类别的正确标签，并计算当前概率分布下的类别交叉熵（例如：假设第一个文档的正确标签是类别 2，我们可以在其概率 $0.6$ 上计算 $\log$ 损失），我们试图告诉模型的是：对于每一个训练样本，我们希望模型对于该样本的正确标签类别分配的概率为 $1.0$。对于概率小于 $1.0$ 的情况，我们基于反向传播计算梯度并且更新模型参数。</p>

<p><strong>预测过程</strong>：<br />
当模型训练完成后，预测过程非常直接：仅涉及单向的向前传播过程。将测试文档转换为词袋表示作为输入向量，然后向前传播，计算得到输出类别的概率分布，并且将其中概率最大值对应的类别标签作为该文档的预测类别。</p>

<p><strong>改进</strong>：</p>
<ul>
  <li>使用 bag-of-ngrams 作为输入。</li>
  <li>文本预处理：词形还原、移除停用词等。</li>
  <li>相比原始计数，我们可以用 TF-IDF 或者指示器（$0$ 或 $1$，取决于该单词是否存在）来进行单词加权。</li>
</ul>

<h3 id="22-作者身份识别">2.2 作者身份识别</h3>
<p>给定一段文本，识别其作者或者作者相关特点（例如：性别、年龄、母语等）。</p>

<ul>
  <li>在该任务中，文本的风格特征要比单词内容本身更重要。
    <ul>
      <li>POS tags 和功能词（例如：$\textit{on, of, the, and}$）</li>
    </ul>
  </li>
  <li>
    <p>关于功能词，我们可以从特定的词典获得（因为功能词通常是一个封闭集合）。或者，另一种关于功能词很好的近似方法是：挑选一个大的语料库中出现频率最高的 300 个单词。</p>
  </li>
  <li><strong>输入</strong>：bag of function words, bag of POS tags, bag of POS bigrams/trigrams</li>
  <li><strong>单词权重</strong>：密度（例如：在一个文本窗口内，功能词和内容词之间的数量比例）</li>
  <li><strong>其他特征</strong>：连续功能词之间距离的分布</li>
</ul>

<h2 id="3-前馈神经网络语言模型">3. 前馈神经网络语言模型</h2>
<p>尽管存在很多不同的下游应用，但是它们对应的语言模型可以是通用的。</p>

<h3 id="31-语言模型回顾">3.1 语言模型回顾</h3>
<ul>
  <li><strong>目标</strong>：为一个单词序列分配概率。</li>
  <li>
    <p><strong>基本框架</strong>：可以视为句子上的 “滑动窗口”，根据有限的上下文对每个单词进行预测。<br />
例如，$n=3$ 时，一个 trigram 模型为：</p>

\[P(w_1,w_2,\dots,w_m)=\prod_{i=1}^{m}P(w_i\mid w_{i-2} w_{i-1})\]
  </li>
  <li>训练（估计）来自频率计数
    <ul>
      <li>难以处理罕见单词 $\to$ 平滑处理</li>
    </ul>
  </li>
</ul>

<h3 id="32-语言模型作为分类器">3.2 语言模型作为分类器</h3>
<p>语言模型可以被视为简单的 <strong>分类器</strong>，对一个序列中下一个可能出现的单词进行分类。</p>

\[P(w_i\mid w_{i-2}=“\textit{cow}”,w_{i-1}=“\textit{eats}”)\]

<p>例如，在 trigram 模型中，我们给定当前的两个上下文单词 “$\textit{cow}$” 和 “$\textit{eats}$”，我们希望预测接下来出现的单词。分类任务是从词汇表（总的类别）中选择可能性最高的一个作为下一个出现的单词。</p>

<h3 id="33-前馈神经网络语言模型">3.3 前馈神经网络语言模型</h3>

<ul>
  <li>
    <p>使用神经网络作为模型的一个分类器</p>

\[P(w_i\mid w_{i-2}=“\textit{cow}”,w_{i-1}=“\textit{eats}”)\]

    <ul>
      <li><strong>输入特征</strong>：前面的两个单词</li>
      <li><strong>输出类别</strong>：下一个单词</li>
    </ul>
  </li>
  <li>
    <p>怎样表示一个单词？<br />
我们可以像之前一样采用 one-hot 向量表示，但是，更好的方法是 <strong>嵌入（Embeddings）</strong>。</p>
  </li>
</ul>

<h3 id="34-词嵌入word-embeddings">3.4 词嵌入（Word Embeddings）</h3>

<ul>
  <li>将离散的单词符号映射为一个相对低维空间中的连续向量。</li>
  <li>词嵌入允许模型捕获单词之间的相似度。
    <ul>
      <li>$\textit{dog}$ vs. $\textit{cat}$</li>
      <li>$\textit{walking}$ vs. $\textit{running}$</li>
    </ul>
  </li>
  <li>
    <p>缓解数据稀疏问题。<br />
例如，考虑语料库中有一个句子：</p>

\[\textit{dog is walking}\]

    <p>现在，我们有一个新的句子：</p>

\[\textit{cat is running}\]

    <p>词嵌入模型对于 “$\textit{dog}$” 和 “$\textit{cat}$” 这两个单词给出了相似的词嵌入，同理，对于 “$\textit{walking}$” 和 “$\textit{running}$” 也给出了相似的词嵌入。然后，我们的模型会意识到既然 “$\textit{dog is walking}$ ” 是一个可能的句子序列，那么，“$\textit{cat is running}$” 也应当是一个可能的句子序列。</p>

    <p>而对于没有采用词嵌入的模型，例如 n-grams 模型，它们很难进行这种泛化，因为在这类模型看来，“$\textit{dog}$” 和 “$\textit{cat}$” 仅仅是两个离散的单词，它们之间并没有什么关联。</p>
  </li>
  <li>
    <p>你可能会问，之前我们表示输入向量 $\mathbf x$ 采用的是词袋模型，而并没有采用词嵌入。事实上，这里词嵌入已经在整个神经网络架构中被隐式地定义了。可以看到，在输入层和第一个隐藏层之间的 <strong>权重向量本质上就是一种嵌入</strong>。</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-18-WX20200418-155602%402x.png" width="50%" /></p>
  </li>
</ul>

<h3 id="35-语言模型架构">3.5 语言模型架构</h3>

<p>那么，怎样用神经网络构建语言模型呢？下图是一个用神经网络构建语言模型的例子。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-18-WX20200418-160139%402x.png" width="80%" /></p>

<p>这里，我们采用 n-grams 语言模型。首先，我们观察 $(n-1)$ 个上下文单词：</p>

\[w_{t-n+1},\dots,w_{t-2},w_{t-1}\]

<p>并对它们进行嵌入，得到相应的词嵌入：</p>

\[C(w_{t-n+1}),\dots,C(w_{t-2}),C(w_{t-1})\]

<p>然后，我们将这些词嵌入连接起来，输入非线性激活函数 $\tanh$ 函数。之后，再进行解除嵌入或者进行另一个线性转换来生成输出层。很重要的一点是，我们试图预测的是一个单词，输出类别的大小实际上等于词汇表的大小，所以输出层中的单元数量实际上等于词汇表中的唯一单词数量。最后，我们在这些值上应用一个 $\text{softmax}$ 函数，从而得到一个关于所有单词概率的向量。之后，当我们要根据上下文预测下一个单词时，我们只需要选择概率向量中的最大值对应的单词作为预测单词即可。</p>

<h3 id="36-例子">3.6 例子</h3>
<p>现在，我们来看一个简单的例子，输入是前面上下文的两个单词 “$\textit{cow}$” 和 “$\textit{eats}$”，然后我们试图估计下一个单词为 “$\textit{grass}$” 的概率：</p>

\[P(w_i=“\textit{grass}”\mid w_{i-2}=“\textit{cow}”,w_{i-1}=“\textit{eats}”)\]

<ul>
  <li>
    <p>查看单词 “$\textit{cow}$” 和 “$\textit{eats}$” 的词嵌入：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-18-WX20200418-200840%402x.png" width="50%" /></p>

    <p>可以看到，在这个例子中，我们的词汇表中大小为 $|V|=5$，词嵌入的维度 $d=4$。</p>
  </li>
  <li>
    <p>将它们连接起来，输入网络中。</p>

\[\mathbf x=\mathbf v_{\textit{cow}}\oplus \mathbf v_{\textit{eats}}\]

\[\mathbf h_1=\tanh(W_1\mathbf x+\mathbf b_1)\]

\[\mathbf y=\text{softmax}(W_2\mathbf h_1)\]

    <p>首先，我们将单词 “$\textit{cow}$” 和 “$\textit{eats}$” 的词嵌入 $\mathbf v_{\textit{cow}}$ 和 $\mathbf v_{\textit{eats}}$ 连接起来作为输入向量 $\mathbf x$。然后通过权重矩阵 $W_1$ 进行加权，并加上偏置向量 $\mathbf b_1$，将结果作为非线性函数 $\tanh$ 的输入，得到隐藏层的表示 $\mathbf h_1$。利用权重矩阵 $W_2$ 对 $\mathbf h_1$ 加权，并作为函数 $\text{softmax}$ 的输入，最终得到预测结果 $\mathbf y$。</p>
  </li>
  <li>
    <p><strong>输出</strong>：$\mathbf y$ 给出了词汇表中所有单词的概率分布。</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-18-WX20200418-202408%402x.png" width="50%" /></p>

\[P(w_i=“\textit{grass}”\mid w_{i-2}=“\textit{cow}”,w_{i-1}=“\textit{eats}”)=0.8\]

    <p>该前馈神经网络模型的大部分参数都在词嵌入（即所有单词的词嵌入表，维度为 $d\times |V|$）和输出嵌入（即 $W_2$，维度为 $|V|\times d$）中。</p>
  </li>
</ul>

<h3 id="37-为什么要用神经网络构建语言模型">3.7 为什么要用神经网络构建语言模型</h3>

<ul>
  <li>N-grams 语言模型
    <ul>
      <li>训练成本低（只需要计算频数）</li>
      <li>稀疏性问题，扩展到更大的上下文时存在问题</li>
      <li>无法充分捕获单词特性（语法和语义上的相似度)，例如：$\textit{film}$ vs. $\textit{movie}$</li>
    </ul>
  </li>
  <li>神经网络语言模型（NNLMs）更具鲁棒性
    <ul>
      <li>强迫将单词用低维的嵌入表示</li>
      <li>自动捕获单词特性，在计算句子概率时，使得估计更具鲁棒性</li>
      <li>灵活性：只需稍加改变就可以适用于其他任务（例如：词性标注）</li>
    </ul>
  </li>
</ul>

<h3 id="38-词性标注">3.8 词性标注</h3>

<ul>
  <li>
    <p>词性标注（POS tagging）也可以被归为分类问题：</p>

    <ul>
      <li>
        <p>例如，给定当前单词 “$\textit{eats}$”，和它的前一个上下文单词 “$\textit{cow}$”，我们的目标是预测当前单词 “$\textit{eats}$” 的词性：</p>

\[P(t_i\mid w_{i-1}=\textit{“cow”},w_{i}=\textit{“eats”})\]

        <p>对当前单词 “$\textit{eats}$” 的可能词性分类。</p>
      </li>
    </ul>
  </li>
  <li>为什么不用一种更时髦的分类器呢？（神经网络）</li>
  <li>神经网络语言模型架构可以直接适用于该任务。</li>
</ul>

<h3 id="39-前馈神经网络用于词性标注">3.9 前馈神经网络用于词性标注</h3>

<ul>
  <li><strong>最大熵马尔可夫模型（Maximum Entropy Markov Model, MEMM）</strong> 标注器：
    <ul>
      <li>输入：
        <ul>
          <li>最近的单词：$w_{i-2},w_{i-1},w_{i}$</li>
          <li>最近的 tag：$t_{i-2},t_{i-1}$</li>
        </ul>
      </li>
      <li>输出：
        <ul>
          <li>当前单词 tag：$t_{i}$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>神经网络</strong> 框架：
    <ul>
      <li>输入：
        <ul>
          <li>5 项：3 个词嵌入，2 个 tag 嵌入</li>
        </ul>
      </li>
      <li>输出：
        <ul>
          <li>1 项：长度为 $|T|$ 的向量（$T$ 为可能的 tags 集合），使用 $\text{softmax}$ 函数</li>
        </ul>
      </li>
      <li>训练过程：
        <ul>
          <li>最小化：$-\sum_{i}\log P(t_i\mid w_{i-2},w_{i-1},w_{i},t_{i-2},t_{i-1})$</li>
        </ul>

        <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-18-WX20200418-235609%402x.png" /></p>

        <p>这里，我们有 3 个单词：$w_{i-2}=\textit{“one”},w_{i-1}=\textit{“small”},w_{i}=\textit{“step”}$，我们将其编码为词嵌入，它们共用一个嵌入矩阵。同时，我们有上下文单词 $w_{i-2},w_{i-1}$ 对应的词性标签：$t_{i-2}=\text{“CD”},t_{i-1}=\text{“JJ”}$，同理，我们将其编码为 tag 嵌入，它们共用另一个嵌入矩阵。然后，我们将得到的 5 个嵌入向量（3 个词嵌入，2 个 tag 嵌入）连接得到一个长的向量，然后作为 $\tanh$ 函数的输入，然后将计算结果传入输出层。输出层的隐藏单元数量等于所有可能的 tags 组成的集合的大小。然后，我们将 $\text{softmax}$ 函数作用于该向量上，然后我们将得到所有可能的 tags 对应的概率向量，然后只需要取其中的最大值对应的 tag 作为当前单词 $w_{i}$ 的词性标签 $t_{i}$ 即可。</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="310-卷积神经网络">3.10 卷积神经网络</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-18-WX20200419-001235%402x.png" /></p>

<p>最后，我们简单介绍一下卷积神经网络。</p>

<ul>
  <li>通常用于 <strong>计算机视觉（Computer Vision, CV）</strong>，但也可以用于 NLP 任务。<br />
在 CV 任务中，输入为一张图片，它由很多 <strong>像素（pixel）</strong>组成。卷积神经网络的工作机制是：每次观察图片中的一小块区域，然后用 <strong>卷积核（convolution kernel）</strong>产生一个特征方阵。对于图片中的每一个小的区域，我们都可以进行这样的操作，这样我们得到了该图片的另一种表示。如果我们采用不同的卷积核，那么将得到不同的图片表示。为什么要使用不同的卷积核呢？因为这样我们可以从不同的潜在角度（例如：颜色、边缘锐度等）来捕获图片特征。之后，我们可以进行 <strong>最大池化（max pooling）</strong>操作，同样，我们观察图片表示中的一小块区域，取其中的最大值作为该区域的表示，对其他图片表示的每个小区域都进行同样的操作，这样我们得到一些新的特征表示。最后，将得到的特征表示喂给一个全连接层，然后我们将得到分类预测的结果。</li>
  <li>识别有代表性特征的局部预测器。<br />
这种方法之所以效果很好的原因在于，相比一次分析整张图片，我们采用了每次只分析其中的一小块区域，本质上看，我们每次只对这一小块区域进行预测，而最大池化操作使得模型可以只保留这块区域最明显的特征。</li>
  <li>将这些局部预测器进行结合，生成一个固定大小的表示。</li>
</ul>

<h3 id="311-卷积神经网络用于-nlp-任务">3.11 卷积神经网络用于 NLP 任务</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-04-18-WX20200419-004045%402x.png" /></p>

<p>这里是一个将卷积神经网络用于 NLP 任务的例子，相比 CV 任务中的像素矩阵，在 NLP 任务中我们的数据是句子。</p>

<ul>
  <li>在整个句子上的滑动窗口（例如：$3$ 个单词）</li>
  <li>$W$ 相当于卷积核（线性变换 + $\tanh$ 函数）</li>
  <li>最大池化生成了一个固定大小的表示</li>
</ul>

<p>我们可以采用和之前相同的思路：<br />
每次观察句子的一小部分，即 <strong>滑动窗口</strong>（例如，这里窗口大小为 $3$ 个单词），窗口每次向前移动一个单词的距离。对于窗口中的每个单词，我们可以将其转换为词嵌入（例如：这里每个词嵌入的维度为 $2$），并且将其连接为一个长的向量（例如：这里该向量长度为 $3\times 2=6$）。然后，对这些连接向量进行卷积核运算（线性变换 + $\tanh$ 函数），我们将得到该窗口对应的一个隐藏特征表示（例如：这里是一个长度为 $3$ 的向量）。对每个滑动窗口进行同样操作，我们将得到一系列的隐藏特征向量。之后，进行最大池化操作，我们观察每个向量中的第一个元素，取其中最大的元素作为输出向量中的对应位置的元素，然后对第二个、第三个元素执行同样的操作。</p>

<p>为什么要进行最大池化操作呢？<br />
有两个原因：1. 由于我们的句子长度是变化的，这样我们得到的特征向量的数量也是变化的，所以，通过最大池化，我们总是可以得到一个固定大小的表示。2. 另一个原因就是之前提到过的，最大池化可以帮助模型识别特征向量中最具代表性的特征。当我们进行 NLP 话题分类任务时，直觉上，我们可以想象某些特定单词对于识别话题是有帮助的，这正是最大池化所做的事情。</p>

<h2 id="4-总结">4. 总结</h2>
<ul>
  <li>神经网络
    <ul>
      <li>鲁棒性（例如：单词变体、拼写错误等）。</li>
      <li>优秀的泛化能力。</li>
      <li>灵活性 —— 基于不同任务定制不同的神经网络架构。</li>
    </ul>
  </li>
  <li>缺点
    <ul>
      <li>训练过程比传统机器学习方法要慢得多，但是可以通过 GPU 加速。</li>
      <li>参数数量很多，主要受词汇表大小、嵌入、网络深度等因素影响。</li>
      <li>对数据量需求很大（data hungry），在小型数据集上表现不是很好。</li>
      <li>在大型语料库上的预训练模型（例如：BERT）可以缓解数据饥饿问题。</li>
    </ul>
  </li>
</ul>

<h2 id="5-扩展阅读">5. 扩展阅读</h2>
<ul>
  <li>Feed-forward network: G15, section 4</li>
  <li>Convolutional network: G15, section 9</li>
</ul>

<p>下节内容：NLP 深度学习：循环网络</p>

:ET