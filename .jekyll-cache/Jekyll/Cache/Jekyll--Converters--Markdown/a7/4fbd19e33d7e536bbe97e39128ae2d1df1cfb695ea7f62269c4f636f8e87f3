I"?<h1 id="module-10-框架和实现">Module 10 框架和实现</h1>

<h2 id="1-ai-伦理框架概览">1. AI 伦理框架概览</h2>

<h3 id="11-引言">1.1 引言</h3>

<p>有许多不同类型和起源的 AI 道德框架：</p>

<ul>
  <li>根据 algorithmwatch.org 的统计，总共有超过 150 个。</li>
  <li>主要由以下机构出版：
    <ul>
      <li>学术界</li>
      <li>文明社会</li>
      <li>政府</li>
      <li>行业协会</li>
      <li>政府间组织</li>
      <li>国际组织</li>
      <li>私营部门</li>
      <li>专业协会</li>
    </ul>
  </li>
  <li>您可以自行探索这些框架：<a href="https://inventory.algorithmwatch.org">https://inventory.algorithmwatch.org</a></li>
</ul>

<p>这些框架的范围从具有约束力的协议到建议。 但是，很少有包含任何执行机制的。</p>

<p>除了来自中国、韩国、日本和印度的框架外，这些框架的起源在很大程度上以西方为中心。美国是迄今为止最大的单一贡献国。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-23-WX20210623-162643%402x.png" width="80%" /></p>

<h3 id="12-展示-a---ieee">1.2 展示 A - IEEE</h3>

<ul>
  <li>IEEE框架（符合道德规范的设计，300 页）</li>
  <li>世界上最大的工程师协会，拥有超过40万名会员</li>
  <li>但是，尽管有许多IEEE成员在职，但像 Google 和 Facebook 这样的公司似乎并没有与他们接触</li>
  <li>OECD 确实基于 IEEE 出版物制定了其原则，那么也许会引发社会化漩涡吗？</li>
</ul>

<h3 id="13-问题">1.3 问题</h3>

<blockquote>
  <p><em>“环境福利、人类代理、透明度，这些只是人工智能伦理框架中通常列出的一些未明确定义的原则，从 Google 到加拿大政府再到 BMW，如今已经有数百种组织发布了这些原则。”</em> $^{[1]}$</p>

  <p><em>“存在什么问题？许多 AI 伦理框架无法在实践中明确实施。”</em> $^{[2]}$</p>
</blockquote>

<ul>
  <li>特异性不足</li>
  <li>难以坚持抽象的高层指导</li>
  <li>人工智能伦理框架是良好的营销</li>
  <li>
    <p>失效</p>
  </li>
  <li>
    <p>技术人员需要伦理资源</p>

    <blockquote>
      <p><em>“如果没有更实际的指导，’道德洗蓝 ‘ 和 ’道德规避 ‘ 等其他风险仍然存在。”</em> $^{[3]}$</p>
    </blockquote>

    <p><strong>伦理洗蓝（Ethics bluewashing）：</strong>故意试图看起来比实际情况更符合伦理。$^{[4]}$</p>

    <p><strong>伦理规避（Ethics shirking）：</strong>在似乎没有高回报的情况下，缺乏对伦理的尊重。$^{[5]}$</p>
  </li>
</ul>

<p><font size="2">[1] Burt, A 2020, ‘Ethical Frameworks for AI Aren’t Enough’, Harvard Business Review Digital</font><br />

<font size="2">[2] Ibid.</font><br />

<font size="2">[3] Morley, J., Floridi, L., Kinsey, L. and Elhalal, A., 2020. From what to how: an initial review of publicly available AI ethics tools, methods and research to translate principles into practices. Science and engineering ethics, 26(4), p 2147.</font><br />

<font size="2">[4, 5] Floridi, L. (2019). Translating principles into practices of digital ethics: Five risks of being unethical. Philosophy &amp; Technology. <a>https://doi.org/10.1007/s13347-019-00354-x</a></font></p>

<h3 id="14-框架原则">1.4 框架原则</h3>

<p>现有人工智能指南中的伦理原则：$^{[6]}$</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-23-WX20210623-170502%402x-1.png" width="80%" /></p>

<font size="2">[6] Jobin, A., Ienca, M. and Vayena, E., 2019. The global landscape of AI ethics guidelines. Nature Machine Intelligence, 1(9), pp.389-399, p 395.</font>

<p><strong>原则主义</strong></p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-23-WX20210623-171506%402x.png" width="50%" /></p>

<h2 id="2-有没有更实用的框架">2. 有没有更实用的框架？</h2>

<h3 id="21-需求">2.1 需求</h3>

<p>为什么在 AI 开发过程中，我们需要伦理框架和指南？</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-23-WX20210623-172747%402x.png" width="80%" /></p>

<p>开发人员如何能够真正确保他们正在开发的 AI 符合伦理？</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-23-WX20210623-174026%402x.png" width="70%" /></p>

<font size="2">来源：Miller C, Coldicutt R. (2019) People, Power and Technology: The Tech Workers’ View, London: Doteveryone. <a>https://doteveryone.org.uk/report/workersview</a> </font>

<h3 id="22-阅读优秀技术的原则">2.2 阅读：优秀技术的原则</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-23-WX20210623-175002%402x.png" width="70%" /></p>

<ul>
  <li>
    <p><strong>PR00：正当性优先（OUGHT BEFORE CAN）</strong>
我们可以做某事并不意味着我们应该做某事。
外面有很多可能的世界 —— 很多东西可以制造或建造。伦理设计是为了确保我们建造的东西有助于创造最好的世界。在我们问是否有可能构建某些东西之前，我们需要先问一下我们为什么要构建它。</p>
  </li>
  <li>
    <p><strong>PR01：非工具主义（NON-INSTRUMENTALISM）</strong>
永远不要设计这样的技术，仅仅将人视为机器的一部分。
有些事情的重要性是无法衡量的，或者说不能只是简单地以实用价值来衡量。人、生态系统、某些动物生命和政治社区不应该被用作可以融入设计的工具。他们必须是您设计的受益者，而不是机器或设计系统的元素。</p>
  </li>
  <li>
    <p><strong>PR02：自决（SELF-DETERMINATION）</strong>
最大限度地提高受您设计影响的人的自由。
技术是人类意志的延伸。它旨在使我们能够实现我们无法实现的目标。如果技术干扰了我们的自由，它就无法实现这一目标。我们需要做出设计选择，以支持人们自由选择他们想要的生活方式和与技术互动的能力。但请记住：最大化自由并不总是意味着最大化选择—— 有时太多的选择可能会导致瘫痪。</p>
  </li>
  <li>
    <p><strong>PR03：责任（RESPONSIBILITY）</strong>
预测和设计所有可能的用途。
技术的设计通常考虑到一个特定的用例（或一组用例）。当用户偏离预期的用例时，往往会出现问题。如果我们花时间仔细考虑，完全有可能预测人们使用我们设计的不同方式。不考虑设计的替代用途及其影响是存在风险且不符合伦理的。这样做可以警示我们防止潜在有害用途，或者通过良好的设计最大化的潜在好处。</p>
  </li>
  <li>
    <p><strong>PR04：净收益（NET BENEFIT）</strong>
最大化好处，最小化坏处。
我们建造的东西应该对世界做出积极的贡献 —— 它们应该让它变得更好。但更重要的是，我们还应该注意我们的技术潜在的有害副作用。即使它的利大于弊，伦理设计也要求我们尽可能减少负面影响。</p>
  </li>
  <li>
    <p><strong>PR05：公平（FAIRNESS）</strong>
以相似的方式对待相似的案例；以不同的方式对待不同的案例。
技术设计可能带有偏见、反映现状，或者产生盲点。这意味着某些人群会因为不相关的偏见或者任意因素，例如种族、年龄、性别、民族或任何数量的不合理考虑等，而遭受负面对待。公平要求我们为设计中存在的对待每个用户群体方式的任何差异提供理由。如果某些群体确实比其他群体遭受更大的伤害或更少的利益，我们必须考虑为什么会出现这种情况，以及我们提供的理由是否站得住脚。</p>
  </li>
  <li>
    <p><strong>PR06：可及性（ACCESSIBILITY）</strong>
设计包含最脆弱的用户。
每当我们确认意向用户概要信息和用例时，我们也会采取行动将非用户从设计考虑中剥离出来。这会导致这样的风险，即设计排除了在流程中考虑可能受益的人。设计可以强化社会劣势，也可以帮助人们克服劣势。但只有当我们牢记所有可能的用户时，它才能做到这一点，而不会将某些群体视为 “边缘情况”。</p>
  </li>
  <li>
    <p><strong>PR07：目的（PURPOSE）</strong>
以诚实、清晰和符合目的的方式进行设计。
从某种意义上说，设计是一种承诺。您承诺解决用户遇到的问题。像所有的承诺一样，你应该兑现这个承诺。您必须诚实并清楚地了解您的设计的能力和局限性。此外，您的设计应该针对它试图解决的问题量身定制 —— 并且旨在解决真正的问题。良好的设计服务于伦理目的，并以高效和有效的方式实现。</p>
  </li>
</ul>

<h3 id="23-框架">2.3 框架</h3>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2021-06-23-WX20210623-183604%402x.png" width="40%" /></p>

<p>Beard 和 Longstaff 谈论了：</p>

<ul>
  <li>框架塑造了技术设计</li>
  <li>框架为要使用的保证过程提供标准</li>
  <li>通过确定证据要求将保证流程纳入设计</li>
  <li>设计为操作功能提供证据</li>
  <li>但是，AI 伦理审查有其自身的问题</li>
</ul>

<h3 id="24-总体问题">2.4 总体问题</h3>

<p>Beard 和 Longstaff 将围绕技术伦理的问题分为三个主要类别：</p>

<ol>
  <li>
    <p><strong>目标：</strong>构建技术的预期目标是什么？</p>
  </li>
  <li>
    <p><strong>使用：</strong>实现公正目标的手段是否可以接受？</p>
  </li>
  <li>
    <p><strong>非预期效应：</strong>需要考虑 <em>可预见（foreseeable）</em>但 <em>非预期（unintended）</em>的效应，并且能够对不可预见的效应做出反应。</p>
  </li>
</ol>

<h3 id="25-意图">2.5 意图</h3>

<blockquote>
  <p>“虽然技术的设计是有意图的，但它的使用只是 ‘部分地’ 受到设计师意图的约束。推动技术设计的创新和天赋也使得（也许鼓励）人工制品的新用法成为可能。在某种程度上，这是因为技术的最佳用途在设计时并非总是显而易见的。”</p>

  <p>—— Beard 和 Longstaff 第 45 页</p>
</blockquote>

<h3 id="26-为什么是原则">2.6 为什么是原则？</h3>

<p>为什么我们需要原则？</p>

<ul>
  <li>它们是行动指南
    <ul>
      <li>允许一系列价值（不同于规则）</li>
      <li>多少具有一些普适性</li>
    </ul>
  </li>
</ul>

<p>它可以帮助我们确定技术在伦理上是否令人满意。</p>

<p>技术设计不仅要符合原则，而且必须以一种明显的方式践行。这些展示应该是：</p>

<ul>
  <li>可用的（可访问的、可理解的）</li>
  <li>能够经得起他人的检查</li>
</ul>

<h2 id="3-回顾设计伦理原则">3. 回顾：设计伦理原则</h2>

<h3 id="31-原则-0正当性优先">3.1 原则 0：正当性优先</h3>

<p><strong>正当性优先（Ought before can）</strong></p>

<blockquote>
  <p>“我们 <em>能够</em> 完成某件事这一事实并不意味着我们 <em>应该</em> 做这件事。”</p>
</blockquote>

<p>Beard 和 Longstaff 将此作为他们框架的指导原则，对此的理解也是本章的第一个学习成果。</p>

<h3 id="32-原则-1非工具主义">3.2 原则 1：非工具主义</h3>

<p><strong>非工具主义（Non-instrumentalism）</strong></p>

<ul>
  <li>将人视为目的，而不仅仅是手段</li>
  <li>以人的尊严为基础</li>
  <li>不要仅仅把人当作工具</li>
  <li>不要操纵人</li>
</ul>

<p>将人降级为 “机器中的齿轮” 或仅作为数据源的设计不是符合伦理的设计。</p>

<h3 id="33-原则-2自决">3.3 原则 2：自决</h3>

<p><strong>自决（Self-determination）</strong></p>

<ul>
  <li>自主权和代理权很重要</li>
  <li>提防会吸引用户或其他人的系统</li>
  <li>即使是好的 “轻推（nudges）” 也应征得用户的同意。</li>
</ul>

<p>问题：谁从 “轻推” 中获益最大？</p>

<p><strong>注：轻推（Nudges/Nudging）</strong>
“轻推” 是指有意识地影响人们所做决定的设计选择。“轻推” 的使用在技术中越来越普遍。 其中一些轻推是透明和明显的，例如当健身应用程序向您发送通知告诉您去散步时。其他的则更为微妙，例如当视频流服务在上一个视频结束后立即自动生成下一个视频时，这实际上重新定义了您选择是否继续观看的性质。</p>

<p>围绕 “轻推” 的伦理考虑是无数的。自由、利益和效率之间的权衡需要逐案评估，以确定特定的轻推是否有意义地增强或削弱了自决。 但是，清楚了解轻推的目的、受益人和影响将有助于确定轻推的伦理特征。</p>

<h3 id="34-原则-3责任">3.4 原则 3：责任</h3>

<p><strong>责任（Responsibility）</strong></p>

<ul>
  <li>尽量减少为了伤害而改变用途的能力
    <ul>
      <li>将其构建到设计中</li>
    </ul>
  </li>
  <li>
    <p>进行改进以减轻</p>
  </li>
  <li>责任存在局限性</li>
  <li>工具在某些特定用途上存在偏见</li>
  <li>但是……可以找到其他用途</li>
  <li>平衡责任需要功利演算</li>
  <li>责任通常是共同承担的</li>
</ul>

<h3 id="35-原则-4净收益">3.5 原则 4：净收益</h3>

<p><strong>净收益（Net benefit）</strong></p>

<ul>
  <li>需要考虑机会成本
    <ul>
      <li>倾向于那些能够提供更多利益的设计</li>
      <li>这需要对这些进行量化的能力……</li>
    </ul>
  </li>
  <li>减少伤害
    <ul>
      <li>以效率为代价</li>
      <li>以有效性为代价</li>
      <li>即使能带来很大利益，也要检查其危害性</li>
    </ul>
  </li>
</ul>

<p>例如，某个系统可以促进世界和平，但是这是通过对每个人的通信进行 24$\times$7 全天候监视而获得的。这确实为我们带来了很大的利益，但是这会给人类的自治和尊严带来什么代价呢？</p>

<h3 id="36-原则-5公平">3.6 原则 5：公平</h3>

<p><strong>公平（Fairness）</strong></p>

<ul>
  <li>一视同仁
    <ul>
      <li>没有任意区别</li>
    </ul>
  </li>
  <li>
    <p>应当考虑偏见</p>
  </li>
  <li>区别对待
    <ul>
      <li>没有错误的对等</li>
    </ul>
  </li>
  <li>
    <p>分享利益和负担</p>
  </li>
  <li>负担最大 $\Longrightarrow$ 收益最大</li>
</ul>

<p>系统公平的局限性应该对所有利益相关者都是透明的。</p>

<h3 id="37-原则-6可及性">3.7 原则 6：可及性</h3>

<p><strong>可及性（Accessibility）</strong></p>

<ul>
  <li>
    <p>设计需要包括最脆弱的用户</p>
  </li>
  <li>参见 Rawls 的 <em>差异原理（Difference Principle）</em>
    <ul>
      <li>确保受益最少的人获得更大的利益</li>
    </ul>
  </li>
  <li>
    <p>您并非总能挑选用户</p>
  </li>
  <li>针对目标市场进行设计（和测试！）可能会加剧现有的不平等</li>
</ul>

<p>例如，仅允许通过智能手机和信用卡进行订购和付款的体育赛事，排除了许多较贫穷或者不熟悉这类技术的人。</p>

<h3 id="38-原则-7目的">3.8 原则 7：目的</h3>

<p><strong>目的（Purpose）</strong></p>

<ul>
  <li>技术应该具有目的性</li>
  <li>符合目的</li>
  <li>适合满足设计目标</li>
</ul>

<p>这个目的应该是：</p>

<ul>
  <li>合法的</li>
  <li>清晰的</li>
  <li>诚实的</li>
  <li>有效的（但仍然合乎道德）</li>
  <li>高效的（但并非不计成本）</li>
</ul>

<h2 id="4-总结">4. 总结</h2>

<p>仅仅有一个框架来指导伦理 AI 的设计和开发是不够的：</p>

<ul>
  <li>您需要关心伦理</li>
  <li>您需要理解有关伦理的知识才能有效地使用框架</li>
  <li>你需要思考、琢磨、反思</li>
  <li>在不道德的环境中更难保持道德</li>
</ul>
:ET