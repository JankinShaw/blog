I"b<h1 id="lecture-16-损失函数-二">Lecture 16 损失函数 (二)</h1>

<p>上节课中，我们学习了损失函数的概念以及四种不同的损失函数。这节课我们继续学习 PyTorch 中提供的另外十四种损失函数。</p>

<h2 id="1-pytorch-中的损失函数">1. PyTorch 中的损失函数</h2>

<p>首先我们来看在回归任务中常用的两个损失函数 <code class="language-plaintext highlighter-rouge">nn.L1Loss</code> 和 <code class="language-plaintext highlighter-rouge">nn.MSELoss</code>：</p>

<h4 id="nnl1loss"><code class="language-plaintext highlighter-rouge">nn.L1Loss</code></h4>

<p><strong>功能</strong>：计算 <code class="language-plaintext highlighter-rouge">inputs</code> 与 <code class="language-plaintext highlighter-rouge">target</code> 之差的绝对值。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">L1Loss</span><span class="p">(</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean</code>。
    <ul>
      <li><code class="language-plaintext highlighter-rouge">none</code>：逐个元素计算。</li>
      <li><code class="language-plaintext highlighter-rouge">sum</code>：所有元素求和，返回标量。</li>
      <li><code class="language-plaintext highlighter-rouge">mean</code>：加权平均，返回标量。</li>
    </ul>
  </li>
</ul>

<p><strong>计算公式</strong>：</p>

\[l_n = |x_n - y_n |\]

<h4 id="nnmseloss"><code class="language-plaintext highlighter-rouge">nn.MSELoss</code></h4>

<p><strong>功能</strong>：计算 <code class="language-plaintext highlighter-rouge">inputs</code> 与 <code class="language-plaintext highlighter-rouge">target</code> 之差的平方。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">(</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean</code>。
    <ul>
      <li><code class="language-plaintext highlighter-rouge">none</code>：逐个元素计算。</li>
      <li><code class="language-plaintext highlighter-rouge">sum</code>：所有元素求和，返回标量。</li>
      <li><code class="language-plaintext highlighter-rouge">mean</code>：加权平均，返回标量。</li>
    </ul>
  </li>
</ul>

<p><strong>计算公式</strong>：</p>

\[l_n = (x_n - y_n )^2\]

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">tools.common_tools</span> <span class="kn">import</span> <span class="n">set_seed</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 设置随机种子
</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="mi">3</span>

<span class="c1"># ------------------------------------ L1 loss ----------------------------------
</span><span class="n">loss_f</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">L1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_f</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"input:{}</span><span class="se">\n</span><span class="s">target:{}</span><span class="se">\n</span><span class="s">L1 loss:{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>

<span class="c1"># ------------------------------------ MSE loss ---------------------------------
</span><span class="n">loss_f_mse</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss_mse</span> <span class="o">=</span> <span class="n">loss_f_mse</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"MSE loss:{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">loss_mse</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>input:tensor([[1., 1.],
        [1., 1.]])
target:tensor([[3., 3.],
        [3., 3.]])
L1 loss:tensor([[2., 2.],
        [2., 2.]])
MSE loss:tensor([[4., 4.],
        [4., 4.]])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可以看到，这里我们的每个神经元的输入为 $x_i = 1$，输出为 $y_i=3$。所以，每个神经元的 L1 loss 为 $|x_i - y_i| = |1-3| = 2$，MSE loss 为 $(x_i - y_i)^2 = (1-3)^2 = 4$。</p>

<h4 id="nnsmoothl1loss"><code class="language-plaintext highlighter-rouge">nn.SmoothL1Loss</code></h4>

<p><strong>功能</strong>：平滑的 L1 Loss，可以减轻离群点带来的影响。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean</code>。
    <ul>
      <li><code class="language-plaintext highlighter-rouge">none</code>：逐个元素计算。</li>
      <li><code class="language-plaintext highlighter-rouge">sum</code>：所有元素求和，返回标量。</li>
      <li><code class="language-plaintext highlighter-rouge">mean</code>：加权平均，返回标量。</li>
    </ul>
  </li>
</ul>

<p><strong>计算公式</strong>：</p>

\[\mathrm{loss}(x,y) = \dfrac{1}{n}\sum_{i=1}^n z_i\]

<p>其中，</p>

\[z_i = \begin{cases}0.5(x_i - y_i)^2\;, &amp; \text{if } |x_i - y_i|&lt; 1 \\[2ex] |x_i - y_i| - 0.5\;, &amp; \text{otherwise}\end{cases}\]

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">loss_f</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss_smooth</span> <span class="o">=</span> <span class="n">loss_f</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="n">loss_l1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">loss_smooth</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s">'Smooth L1 Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">loss_l1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'L1 loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'x_i - y_i'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'loss value'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-24-smooth.png" width="60%" /></p>

<h4 id="poissonnllloss"><code class="language-plaintext highlighter-rouge">PoissonNLLLoss</code></h4>

<p><strong>功能</strong>：泊松分布的负对数似然损失函数。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">PoissonNLLLoss</span><span class="p">(</span>
    <span class="n">log_input</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">full</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">log_input</code>：输入是否为对数形式，决定计算公式。</li>
  <li><code class="language-plaintext highlighter-rouge">full</code>：计算所有 loss，默认为 <code class="language-plaintext highlighter-rouge">False</code>。</li>
  <li><code class="language-plaintext highlighter-rouge">eps</code>：修正项，避免 <code class="language-plaintext highlighter-rouge">input</code> 为 $0$ 时，<code class="language-plaintext highlighter-rouge">log(input)</code> 为 <code class="language-plaintext highlighter-rouge">nan</code> 的情况。</li>
</ul>

<p><strong>计算公式</strong>：</p>

<ul>
  <li>
    <p>当 <code class="language-plaintext highlighter-rouge">log_input=True</code> 时：</p>

\[\mathrm{loss}(x_n, y_n) = e^{x_n} - x_n \cdot y_n\]
  </li>
  <li>
    <p>当 <code class="language-plaintext highlighter-rouge">log_input=False</code> 时：</p>

\[\mathrm{loss}(x_n, y_n) = x_n - y_n \cdot \log(x_n + \mathrm{eps})\]
  </li>
</ul>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">loss_f</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">PoissonNLLLoss</span><span class="p">(</span><span class="n">log_input</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_f</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"input:{}</span><span class="se">\n</span><span class="s">target:{}</span><span class="se">\n</span><span class="s">Poisson NLL loss:{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>input:tensor([[0.6614, 0.2669],
        [0.0617, 0.6213]])
target:tensor([[-0.4519, -0.1661],
        [-1.5228,  0.3817]])
Poisson NLL loss:tensor([[2.2363, 1.3503],
        [1.1575, 1.6242]])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>下面我们以第一个神经元的 loss 为例，通过手动计算来验证我们前面的公式是否正确：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">loss_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">])</span> <span class="o">-</span> <span class="n">target</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span><span class="o">*</span><span class="n">inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">"第一个元素的 loss 为:"</span><span class="p">,</span> <span class="n">loss_1</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>第一个元素的 loss 为: tensor(2.2363)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可以看到，由于这里我们的 <code class="language-plaintext highlighter-rouge">log_input=True</code>，默认输入为对数形式，计算出的第一个神经元的 loss 为 $2.2363$，与前面 PyTorch 中 <code class="language-plaintext highlighter-rouge">nn.PoissonNLLLoss</code> 的计算结果一致。</p>

<h4 id="nnkldivloss"><code class="language-plaintext highlighter-rouge">nn.KLDivLoss</code></h4>

<p><strong>功能</strong>：计算 KL 散度 (KL divergence, KLD)，即相对熵。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">KLDivLoss</span><span class="p">(</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean/batchmean</code>。
    <ul>
      <li><code class="language-plaintext highlighter-rouge">none</code>：逐个元素计算。</li>
      <li><code class="language-plaintext highlighter-rouge">sum</code>：所有元素求和，返回标量。</li>
      <li><code class="language-plaintext highlighter-rouge">mean</code>：加权平均，返回标量。</li>
      <li><code class="language-plaintext highlighter-rouge">batchmean</code>：<code class="language-plaintext highlighter-rouge">batchsize</code> 维度求平均值。</li>
    </ul>
  </li>
</ul>

<p><strong>计算公式</strong>：</p>

\[\begin{aligned}
D_{\mathrm{KL}}(P,Q) = \mathrm{E}_{X\sim P}\left[\log \dfrac{P(X)}{Q(X)}\right] &amp;= \mathrm{E}_{X\sim P}[\log P(X) - \log Q(X)] \\[2ex]
&amp;= \sum_{i=1}^{n} P(x_i)(\log P(x_i) - \log Q(x_i))
\end{aligned}\]

<p>其中，$P$ 为数据的真实分布，$Q$ 为模型拟合的分布。</p>

<p>PyTorch 中的计算公式：</p>

\[l_n = y_n \cdot (\log y_n - x_n)\]

<p>由于 PyTorch 是逐个元素计算的，因此可以移除 $\Sigma$ 求和项。而括号中第二项这里是 $x_n$，而不是 $\log Q(x_n)$，因此我们需要提前计算输入的对数概率。</p>

<p><strong>注意事项</strong>：需提前将输入计算 log-probabilities，例如通过 <code class="language-plaintext highlighter-rouge">nn.logsoftmax()</code> 计算。</p>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>
<span class="n">inputs_log</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

<span class="n">loss_f_none</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss_f_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">)</span>
<span class="n">loss_f_bs_mean</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'batchmean'</span><span class="p">)</span>

<span class="n">loss_none</span> <span class="o">=</span> <span class="n">loss_f_none</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss_mean</span> <span class="o">=</span> <span class="n">loss_f_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss_bs_mean</span> <span class="o">=</span> <span class="n">loss_f_bs_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"loss_none:</span><span class="se">\n</span><span class="s">{}</span><span class="se">\n</span><span class="s">loss_mean:</span><span class="se">\n</span><span class="s">{}</span><span class="se">\n</span><span class="s">loss_bs_mean:</span><span class="se">\n</span><span class="s">{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">loss_none</span><span class="p">,</span> <span class="n">loss_mean</span><span class="p">,</span> <span class="n">loss_bs_mean</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre>loss_none:
tensor([[-0.5448, -0.1648, -0.1598],
        [-0.2503, -0.4597, -0.4219]])
loss_mean:
-0.3335360586643219
loss_bs_mean:
-1.000608205795288
</pre></td></tr></tbody></table></code></pre></div></div>

<p>由于我们的输入是一个 $2\times 3$ 的 Tensor，所以我们的 loss 也是一个 $2\times 3$ 的 Tensor。在 <code class="language-plaintext highlighter-rouge">mean</code> 模式下，我们得到 6 个 loss 的均值为 $-0.3335$；而 <code class="language-plaintext highlighter-rouge">batchmean</code> 模式下是 6 个 loss 相加再除以 $2$，所以得到 $-1.0006$。</p>

<p>下面我们以第一个神经元的 loss 为例，通过手动计算来验证 PyTorch 中的公式是否和我们之前提到的一致：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">loss_1</span> <span class="o">=</span> <span class="n">target</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">])</span> <span class="o">-</span> <span class="n">inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">])</span>  <span class="c1">#  注意，这里括号中第二项没有取 log
</span>
<span class="k">print</span><span class="p">(</span><span class="s">"第一个元素的 loss 为:"</span><span class="p">,</span> <span class="n">loss_1</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>第一个元素的 loss 为: tensor(-0.5448)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可以看到，手动计算的结果与前面 PyTorch 中的 <code class="language-plaintext highlighter-rouge">nn.KLDivLoss</code> 的结果一致。</p>

<h4 id="nnmarginrankingloss"><code class="language-plaintext highlighter-rouge">nn.MarginRankingLoss</code></h4>

<p><strong>功能</strong>：计算两个向量之间的相似度，用于 <strong>排序任务</strong>。该方法计算两组数据之间的差异，返回一个 $n\times n$ 的 loss 矩阵。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span>
    <span class="n">margin</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">margin</code>：边界值，$x_1$ 与 $x_2$ 之间的差异值。</li>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean</code>。</li>
</ul>

<p><strong>计算公式</strong>：</p>

\[\mathrm{loss}(x,y)= \max (0, -y \cdot (x_1 - x_2) + \mathrm{margin})\]

<ul>
  <li>当 $y=1$ 时，我们希望 $x_1$ 比 $x_2$ 大，当 $x_1 &gt; x_2$ 时， 不产生 loss。</li>
  <li>当 $y=-1$ 时，我们希望 $x_2$ 比 $x_1$ 大，当 $x_2 &gt; x_1$ 时，不产生 loss。</li>
</ul>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

<span class="n">loss_f_none</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MarginRankingLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_f_none</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre>tensor([[1., 1., 0.],
        [0., 0., 0.],
        [0., 0., 1.]])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>由于这里我们的输入是两个长度为 $3$ 的向量，所以输出的是一个 $3\times 3$ 的 loss 矩阵。该矩阵中的第一行是由 <code class="language-plaintext highlighter-rouge">x1</code> 中的第一个元素与 <code class="language-plaintext highlighter-rouge">x2</code> 中的三个元素计算得到的 loss。当 $y=1$ 时，$x_1 =1$，$x_2=2$，$x_1$ 并没有大于 $x_2$，因此会产生 loss 为 $\max (0, -1\times (1 - 2)) = 1$，所以输出矩阵中第一行的第一个元素为 $1$；第一行中的第二个元素同理。对于第一行中的第三个元素，$y = -1$，$x_1 =1$，$x_2=2$，满足 $x_2 &gt; x_1$，因此不会产生 loss，即 loss 为 $\max (0, 1\times (1 - 2)) = 0$，所以输出矩阵中第一行的第三个元素为 $0$。</p>

<h4 id="nnmultilabelmarginloss"><code class="language-plaintext highlighter-rouge">nn.MultiLabelMarginLoss</code></h4>

<p><strong>功能</strong>：多标签边界损失函数。例如四分类任务，样本 $x$ 属于第 $0$ 类和第 $3$ 类，注意这里标签为 $[0,3,-1,-1]$，而不是 $[1,0,0,1]$。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">MultiLabelMarginLoss</span><span class="p">(</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean</code>。</li>
</ul>

<p><strong>计算公式</strong>：</p>

\[\mathrm{loss}(x,y) = \sum_{ij}\dfrac{\max(0, 1-x[ y [j] ] - x[i])}{x.\mathrm{size}(0)}\]

<p>其中，$i=0,\dots,x.\mathrm{size}(0)\;,\;j=0,\dots,y.\mathrm{size}(0)$，对于所有的 $i$ 和 $j$，都有 $y[j] \ge 0$ 并且 $i \ne y[j]$。</p>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>  <span class="c1"># 一个四分类样本的输出概率
</span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>  <span class="c1"># 标签，该样本属于第 0 类和第 3 类
</span>
<span class="n">loss_f</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MultiLabelMarginLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>tensor([0.8500])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>下面我们通过手动计算来验证前面计算公式的正确性：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">item_1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>    <span class="c1"># 第 0 类标签的 loss
</span><span class="n">item_2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>    <span class="c1"># 第 3 类标签的 loss
</span>
<span class="n">loss_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">item_1</span> <span class="o">+</span> <span class="n">item_2</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_h</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>tensor(0.8500)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可以看到，手动计算的结果与 PyTorch 中的 <code class="language-plaintext highlighter-rouge">nn.MultiLabelMarginLoss</code> 的结果一致。</p>

<h4 id="nnsoftmarginloss"><code class="language-plaintext highlighter-rouge">nn.SoftMarginLoss</code></h4>

<p><strong>功能</strong>：计算二分类的 logistic 损失。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">SoftMarginLoss</span><span class="p">(</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean</code>。</li>
</ul>

<p><strong>计算公式</strong>：</p>

\[\mathrm{loss}(x,y) = \sum_i \dfrac{\log(1 + \exp (-y[i] \cdot x[i]))}{x.\mathrm{nelement()}}\]

<p>其中，$x.\mathrm{nelement()}$ 为输入 $x$ 中的样本个数。注意这里 $y$ 也有 $1$ 和 $-1$ 两种模式。</p>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>  <span class="c1"># 两个样本，两个神经元
</span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>  <span class="c1"># 该 loss 为逐个神经元计算，需要为每个神经元单独设置标签
</span>
<span class="n">loss_f</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">SoftMarginLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_f</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"SoftMargin: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>SoftMargin:  tensor([[0.8544, 0.4032],
        [0.4741, 0.9741]])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>下面我们以第一个神经元的 loss 为例，采用手动计算来验证上面公式的正确性：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">inputs_i</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span>
<span class="n">target_i</span> <span class="o">=</span> <span class="n">target</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span>

<span class="n">loss_h</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">target_i</span> <span class="o">*</span> <span class="n">inputs_i</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_h</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>tensor(0.8544)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可以看到，手动计算的结果与 PyTorch 中的 <code class="language-plaintext highlighter-rouge">nn.SoftMarginLoss</code> 的结果一致。</p>

<h4 id="nnmultilabelsoftmarginloss"><code class="language-plaintext highlighter-rouge">nn.MultiLabelSoftMarginLoss</code></h4>

<p><strong>功能</strong>：<code class="language-plaintext highlighter-rouge">SoftMarginLoss</code> 的多标签版本。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">MultiLabelSoftMarginLoss</span><span class="p">(</span>
    <span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">weight</code>：各类别的 loss 设置权值。</li>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean</code>。</li>
</ul>

<p><strong>计算公式</strong>：</p>

\[\mathrm{loss}(x,y)= - \dfrac{1}{C} \cdot \sum_i y[i] \cdot \log\left(\dfrac{1}{1+\exp(-x[i]))} \right) + (1-y[i]) \cdot \log \left(\dfrac{\exp(-x[i])}{1+\exp(-x[i]))} \right)\]

<p>其中，$C$ 是标签类别的数量，$i$ 表示第 $i$ 个神经元，这里标签取值为 $0$ 或者 $1$，例如在一个四分类任务中，某样本标签为第 $0$ 类和第 $3$ 类，那么该样本的标签向量为 $[1,0,0,1]$。</p>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

<span class="n">loss_f</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MultiLabelSoftMarginLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_f</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"MultiLabel SoftMargin: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>MultiLabel SoftMargin:  tensor([0.5429])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>下面我们通过手动计算验证上面公式的正确性：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">i_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])))</span>
<span class="n">i_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])))</span>
<span class="n">i_2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])))</span>

<span class="n">loss_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">i_0</span> <span class="o">+</span> <span class="n">i_1</span> <span class="o">+</span> <span class="n">i_2</span><span class="p">)</span> <span class="o">/</span> <span class="o">-</span><span class="mi">3</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_h</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>tensor(0.5429)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可以看到，手动计算的结果与 PyTorch 中的 <code class="language-plaintext highlighter-rouge">nn.MultiLabelSoftMarginLoss</code> 的结果一致。</p>

<h4 id="nnmultimarginloss"><code class="language-plaintext highlighter-rouge">nn.MultiMarginLoss</code></h4>

<p><strong>功能</strong>：计算多分类的折页损失。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">MultiMarginLoss</span><span class="p">(</span>
    <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">p</code>：可选 <code class="language-plaintext highlighter-rouge">1</code> 或 <code class="language-plaintext highlighter-rouge">2</code>。</li>
  <li><code class="language-plaintext highlighter-rouge">weight</code>：各类别的 loss 设置权值。</li>
  <li><code class="language-plaintext highlighter-rouge">margin</code>：边界值。</li>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean</code>。</li>
</ul>

<p><strong>计算公式</strong>：</p>

\[\mathrm{loss}(x,y) = \dfrac{\sum_i \max(0,\mathrm{margin}-x[y] + x[i])^p}{x.\mathrm{size(0)}}\]

<p>其中，$x\in \{0,\dots,x.\mathrm{size}(0)-1\}\;,\; y\in \{0,\dots,y.\mathrm{size}(0)-1\}$，并且对于所有的 $i$ 和 $j$，都有 $0 \le y[j] \le x.\mathrm{size}(0)-1$，以及 $i \ne y[j]$。</p>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>

<span class="n">loss_f</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MultiMarginLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Multi Margin Loss: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Multi Margin Loss:  tensor([0.8000, 0.7000])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>下面我们以第一个样本的 loss 为例，通过手动计算验证上面公式的正确性：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">margin</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">i_0</span> <span class="o">=</span> <span class="n">margin</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">i_2</span> <span class="o">=</span> <span class="n">margin</span> <span class="o">-</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">loss_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">i_0</span> <span class="o">+</span> <span class="n">i_2</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_h</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>tensor(0.8000)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可以看到，手动计算的结果与 PyTorch 中的 <code class="language-plaintext highlighter-rouge">nn.MultiMarginLoss</code> 的结果一致。</p>

<h4 id="nntripletmarginloss"><code class="language-plaintext highlighter-rouge">nn.TripletMarginLoss</code></h4>

<p><strong>功能</strong>：计算三元组损失，常用于人脸识别验证。</p>

<p>三元组损失：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-25-WX20201225-114420%402x.png" width="60%" /></p>

<p>我们希望通过学习，使得 Anchor 与 Posttive 之间的距离小于 Anchor 与 Negative 之间的距离。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">TripletMarginLoss</span><span class="p">(</span>
    <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span>
    <span class="n">swap</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">p</code>：范数的阶，默认为 <code class="language-plaintext highlighter-rouge">2</code>。</li>
  <li><code class="language-plaintext highlighter-rouge">margin</code>：边界值。</li>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean</code>。</li>
</ul>

<p><strong>计算公式</strong>：</p>

\[L(a,p,n) = \max \{d(a_i, p_i) - d(a_i, n_i) + \mathrm{margin}, 0\}\]

<p>其中，$d(x_i, y_i) = \|\mathbf{x}_i - \mathbf{y}_i \|_p$。</p>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="n">anchor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">]])</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.</span><span class="p">]])</span>
<span class="n">neg</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">]])</span>

<span class="n">loss_f</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">TripletMarginLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 范数为 1，即计算两者之差的绝对值
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_f</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">neg</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Triplet Margin Loss"</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Triplet Margin Loss tensor(1.5000)
</pre></td></tr></tbody></table></code></pre></div></div>

<p>下面我们通过手动计算验证上面公式的正确性：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="n">margin</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">a</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">anchor</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">neg</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">d_ap</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">p</span><span class="p">)</span>
<span class="n">d_an</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">n</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">d_ap</span> <span class="o">-</span> <span class="n">d_an</span> <span class="o">+</span> <span class="n">margin</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>tensor([1.5000])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可以看到，手动计算的结果与 PyTorch 中的 <code class="language-plaintext highlighter-rouge">nn.TripletMarginLoss</code> 的结果一致。</p>

<h4 id="nnhingeembeddingloss"><code class="language-plaintext highlighter-rouge">nn.HingeEmbeddingLoss</code></h4>

<p><strong>功能</strong>：计算两个输入的相似性，常用于非线性 embedding 和半监督学习。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">HingeEmbeddingLoss</span><span class="p">(</span>
    <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">margin</code>：边界值。</li>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean</code>。</li>
</ul>

<p><strong>计算公式</strong>：</p>

\[l_n = \begin{cases}x_n\, , &amp; \text{if } y_n = 1 \\[2ex] \max\{0,\Delta - x_n\}\, , &amp; \text{if } y_n = -1\end{cases}\]

<p><strong>注意事项</strong>：输入 $x$ 应为两个输入之差的绝对值。</p>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">loss_f</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">HingeEmbeddingLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_f</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Hinge Embedding Loss"</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Hinge Embedding Loss tensor([[1.0000, 0.8000, 0.5000]])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>下面我们通过手动计算验证上面公式的正确性：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="n">margin</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">loss_0</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">loss_1</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">loss_2</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">margin</span> <span class="o">-</span> <span class="n">inputs</span><span class="p">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss_0</span><span class="p">,</span> <span class="n">loss_1</span><span class="p">,</span> <span class="n">loss_2</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>1.0 0.8 0.5
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可以看到，手动计算的结果与 PyTorch 中的 <code class="language-plaintext highlighter-rouge">nn.HingeEmbeddingLoss</code> 的结果一致。</p>

<h4 id="nncosineembeddingloss"><code class="language-plaintext highlighter-rouge">nn.CosineEmbeddingLoss</code></h4>

<p><strong>功能</strong>：采用余弦相似度计算两个输入的相似性。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">CosineEmbeddingLoss</span><span class="p">(</span>
    <span class="n">margin</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="nb">reduce</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">margin</code>：可取值 $[-1, 1]$，推荐为 $[0, 0.5]$。</li>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean</code>。</li>
</ul>

<p><strong>计算公式</strong>：</p>

\[\mathrm{loss}(x,y) = \begin{cases}1-\cos(x_1,x_2)\, , &amp; \text{if } y = 1 \\[2ex] \max\{0,\cos(x_1,x_2)-\mathrm{margin}\}\, , &amp; \text{if } y = -1\end{cases}\]

<p>其中，</p>

\[\cos(\theta) = \dfrac{A\cdot B}{\|A\|  \|B\|} = \dfrac{\sum_{i=1}^{n}A_i \times B_i}{\sqrt{\sum_{i=1}^{n}(A_i)^2} \times \sqrt{\sum_{i=1}^{n}(B_i)^2}}\]

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]])</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

<span class="n">loss_f</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CosineEmbeddingLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_f</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Cosine Embedding Loss"</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Cosine Embedding Loss tensor([[0.0167, 0.9833]])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>下面我们通过手动计算验证上面公式的正确性：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="n">margin</span> <span class="o">=</span> <span class="mf">0.</span>

<span class="k">def</span> <span class="nf">cosine</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">numerator</span><span class="o">/</span><span class="n">denominator</span><span class="p">)</span>

<span class="n">l_1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">cosine</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x2</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">l_2</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">cosine</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="n">margin</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">l_1</span><span class="p">,</span> <span class="n">l_2</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>0.016662120819091797 0.9833378791809082
</pre></td></tr></tbody></table></code></pre></div></div>

<p>可以看到，手动计算的结果与 PyTorch 中的 <code class="language-plaintext highlighter-rouge">nn.CosineEmbeddingLoss</code> 的结果一致。</p>

<h4 id="nnctcloss"><code class="language-plaintext highlighter-rouge">nn.CTCLoss</code></h4>
<p><strong>参考文献</strong>：<a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf">A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</a></p>

<p><strong>功能</strong>：计算 CTC (Connectionist Temporal Classification) 损失，用于解决时序类数据的分类。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">CTCLoss</span><span class="p">(</span>
    <span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s">'mean'</span><span class="p">,</span>
    <span class="n">zero_infinity</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">blank</code>：blank label。</li>
  <li><code class="language-plaintext highlighter-rouge">zero_infinity</code>：无穷大的值或梯度值为 $0$。</li>
  <li><code class="language-plaintext highlighter-rouge">reduction</code>：计算模式，可为 <code class="language-plaintext highlighter-rouge">none/sum/mean</code>。</li>
</ul>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="n">T</span> <span class="o">=</span> <span class="mi">50</span>      <span class="c1"># Input sequence length
</span><span class="n">C</span> <span class="o">=</span> <span class="mi">20</span>      <span class="c1"># Number of classes (including blank)
</span><span class="n">N</span> <span class="o">=</span> <span class="mi">16</span>      <span class="c1"># Batch size
</span><span class="n">S</span> <span class="o">=</span> <span class="mi">30</span>      <span class="c1"># Target sequence length of longest target in batch
</span><span class="n">S_min</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Minimum target length, for demonstration purposes
</span>
<span class="c1"># Initialize random batch of input vectors, for *size = (T,N,C)
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">).</span><span class="n">log_softmax</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">detach</span><span class="p">().</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="c1"># Initialize random batch of targets (0 = blank, 1:C = classes)
</span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>

<span class="n">input_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,),</span> <span class="n">fill_value</span><span class="o">=</span><span class="n">T</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
<span class="n">target_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">S_min</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">S</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>

<span class="n">ctc_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CTCLoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">ctc_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"CTC loss: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>CTC loss:  tensor(7.5385, grad_fn=&lt;MeanBackward0&gt;)
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="2-总结">2. 总结</h2>

<p>到这里，我们已经学习完了 PyTorch 中的 18 种损失函数：</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.NLLLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.BCELoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.BCEWithLogitsLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.L1Loss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.MSELoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.SmoothL1Loss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.PoissonNLLLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.KLDivLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.MarginRankingLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.MultiLabelMarginLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.SoftMarginLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.MultiLabelSoftMarginLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.MultiMarginLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.TripletMarginLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.HingeEmbeddingLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.CosineEmbeddingLoss</code></li>
  <li><code class="language-plaintext highlighter-rouge">nn.CTCLoss</code></li>
</ol>

<p>下节课中，我们将学习 PyTorch 中的优化器。</p>

<p>下节内容：优化器 (一)</p>
:ET