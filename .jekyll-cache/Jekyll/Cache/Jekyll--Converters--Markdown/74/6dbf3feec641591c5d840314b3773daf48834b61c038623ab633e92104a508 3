I"R3<h1 id="lecture-2-统计思想流派">Lecture 2 统计思想流派</h1>
<h2 id="主要内容">主要内容</h2>
<ul>
  <li><strong>学习算法是如何产生的？</strong>
    <ul>
      <li>频率统计学</li>
      <li>统计决策理论</li>
      <li>贝叶斯统计学</li>
    </ul>
  </li>
  <li><strong>概率模型的类型</strong>
    <ul>
      <li>参数模型 vs. 非参数模型</li>
      <li>生成式模型 vs. 判别式模型</li>
    </ul>
  </li>
</ul>

<h2 id="1-频率统计学">1. 频率统计学</h2>
<p><strong>其中未知的模型参数被视为具有固定但未知的值。</strong></p>

<h3 id="11-频率统计学">1.1 频率统计学</h3>
<ul>
  <li>抽象问题
    <ul>
      <li>已知：$X_1,X_2,…,X_n$ 来自某未知的独立同分布（i.i.d.）</li>
      <li>目标：识别该未知分布，或者其性质</li>
    </ul>
  </li>
  <li>参数方法（“参数估计”）
    <ul>
      <li>模型 $\{p_\theta(x):\theta \in \Theta\}$ 的类别由参数 $\Theta$ （可以是实数或者向量等）索引</li>
      <li>点估计 $\hat \theta (x_1,x_2,…,x_n)$ 是数据的一个函数（或者统计量）</li>
    </ul>
  </li>
  <li>例子
    <ul>
      <li>抛 $n$ 次硬币，求正面朝上的概率</li>
      <li>选择一种分类器</li>
    </ul>
  </li>
</ul>

<h3 id="12-偏差方差和渐近变体">1.2 偏差、方差和渐近变体</h3>
<p>频率学家在理想条件下寻求好的表现</p>
<ul>
  <li>偏差：$B_\theta(\hat\theta)=E_\theta[\hat\theta(X_1,…,X_n)]-\theta$</li>
  <li>方差：$Var_\theta(\hat\theta)=E_\theta[(\hat\theta - E_\theta[\hat\theta])^2]$</li>
</ul>

<p>渐近性质</p>
<ul>
  <li>相合性：当 $n \rightarrow \infty$ 时，$\hat\theta(X_1,…,X_n)$ 收敛于 $\theta$</li>
  <li>有效性：渐近方差尽可能小</li>
</ul>

<h3 id="13-最大似然估计mle">1.3 最大似然估计（MLE）</h3>
<ul>
  <li>一个设计估计量的一般原则</li>
  <li>涉及优化</li>
  <li>$\hat\theta(x_1,…,x_n) \in \mathop{\operatorname{arg\,max}}\limits_{\theta\in\Theta} \prod_{i=1}^{n}p_\theta(x_i)$ （乘积是因为假设数据点之间互相独立）</li>
  <li>MLE 估计量是相合的（在一定条件下）</li>
</ul>

<h4 id="例子-1伯努利分布">例子 1：伯努利分布</h4>
<ul>
  <li>已知数据来自参数未知的伯努利分布（例如偏向硬币），找出该分布的均值</li>
  <li>均值的 MLE 估计
    <ul>
      <li>$p_\theta(x)=\begin{cases}\theta,\quad \text{if }x=0\\1-\theta,\quad \text{if }x=1\end{cases}=\theta^x (1-\theta)^{1-x}$
(注意：对于所有其他的 $x$，$p_\theta(x)=0$)</li>
      <li>最大化似然函数得到：$\hat\theta=\dfrac{1}{n}\sum_{i=1}^n X_i$</li>
    </ul>
  </li>
</ul>

<h4 id="例子-2正态分布">例子 2：正态分布</h4>
<ul>
  <li>已知数据来自方差为 1 但均值未知的正态分布，找出该分布的均值</li>
  <li>均值的 MLE 估计
    <ul>
      <li>$p_\theta(x)=\frac{1}{\sqrt{2\pi}}\exp(-\frac{1}{2}(x-\theta)^2)$</li>
      <li>最大化似然函数得到：$\hat\theta=\dfrac{1}{n}\sum_{i=1}^n X_i$</li>
    </ul>
  </li>
  <li>练习：证明当方差为 $\sigma^2$ 时，MLE 似然函数为 $p_\theta(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2\sigma^2}(x-\mu)^2)$，这里 $\theta=(\mu,\sigma^2)$</li>
</ul>

<h4 id="mle-算法">MLE “算法”</h4>
<ol>
  <li>给定数据 $X_1,…,X_n$ 定义概率分布 $p_\theta$，假设数据是由该分布生成的</li>
  <li>写出数据的似然函数 $\prod_{i=1}^n p_\theta(X_i)$ （为了方便计算，通常取其对数）</li>
  <li>通过优化找到最优（使得数据可能性最大）的参数 $\hat\theta$
    <ul>
      <li>将对数似然函数对 $\theta$ 求偏导；令其等于 0，求解</li>
      <li>如果不行，使用迭代梯度法</li>
    </ul>
  </li>
</ol>

<h2 id="2-统计决策理论">2. 统计决策理论</h2>
<p><strong>是统计学、优化理论、经济学、控制理论的分支，强调效用最大化。</strong></p>

<h3 id="21-决策理论">2.1 决策理论</h3>
<ul>
  <li>采取行动以最大化效用 - 与经济学和运筹学相关</li>
  <li>决策规则 $\delta(\boldsymbol x) \in A$，其中 $A$ 表示行动空间
    <ul>
      <li>例如：点估计 $\hat\theta(x_1,…,x_n)$</li>
      <li>例如：样本外预测 $\hat Y_{n+1}\mid X_1,Y_1,…,X_n,Y_n,X_{n+1}$</li>
    </ul>
  </li>
  <li>损失函数 $l(a,\theta)$：经济成本、误差度量
    <ul>
      <li>例如：平方损失估计 $(\hat\theta - \theta)^2$</li>
      <li>例如：分类器预测结果的 0-1 损失 $1[y\ne \hat y]$</li>
    </ul>
  </li>
</ul>

<h3 id="22-风险与经验风险最小化erm">2.2 风险与经验风险最小化（ERM）</h3>
<ul>
  <li>在决策理论中，我们真正关心的是期望损失</li>
  <li>损失 $R_\theta[\delta]=E_{\boldsymbol{X}\sim \theta}[l(\delta(\boldsymbol{X}),\theta)]$
    <ul>
      <li>例如：真实测试误差，或者泛化误差</li>
    </ul>
  </li>
  <li>目标：选择 $\delta$ 使得 $R_\theta[\delta]$ 最小</li>
  <li>无法直接完成，为什么？</li>
  <li>ERM：使用训练集 $\boldsymbol{X}$ 来近似估计 $p_\theta$
    <ul>
      <li>经验风险最小化 $\hat R_\theta[\delta]=\frac{1}{n}\sum_{i=1}^n l(\delta(X_i),\theta)$</li>
    </ul>
  </li>
</ul>

<h3 id="23-展望-lecture-3">2.3 展望 Lecture 3</h3>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8cxn7949mj30n40m2add.jpg" width="50%" /></p>

<ul>
  <li>优化与机器学习
    <ul>
      <li>最大似然估计</li>
      <li>经验风险最小化</li>
      <li>…</li>
    </ul>
  </li>
</ul>

<h3 id="24-偏差-方差分解">2.4 偏差-方差分解</h3>
<ul>
  <li>偏差：$B_\theta(\hat\theta)=E_\theta[\hat\theta(X_1,…,X_n)]-\theta$</li>
  <li>方差：$Var_\theta(\hat\theta)=E_\theta[(\hat\theta - E_\theta[\hat\theta])^2]$</li>
  <li>平方损失风险的偏差-方差分解
$E_\theta[(\theta - \hat \theta)^2]=[B(\hat \theta)]^2+Var_\theta (\hat \theta)$</li>
</ul>

<h2 id="3-贝叶斯统计学">3. 贝叶斯统计学</h2>
<p><strong>其中未知的模型参数具有反映先验信念的关联分布。</strong></p>

<h3 id="31-贝叶斯统计学">3.1 贝叶斯统计学</h3>
<ul>
  <li>概率对应信念</li>
  <li>参数
    <ul>
      <li>对随机变量的分布建模</li>
      <li>对 $\theta$ 的先验信念由先验分布 $P(\theta)$ 编码
        <ul>
          <li>同随机变量类似，对参数建模（即使其并非真的随机）</li>
          <li>因此，数据的似然函数 $P_\theta(X)$ 写成条件概率的形式 $P(X\mid \theta)$</li>
        </ul>
      </li>
      <li>不同于点估计 $\hat\theta$，贝叶斯会结合观测数据，将先验分布 $P(\theta)$ 更新为后验分布 $P(\theta\mid X)$</li>
    </ul>
  </li>
</ul>

<h3 id="32-概率推断工具">3.2 概率推断工具</h3>
<ul>
  <li>贝叶斯统计推断
    <ul>
      <li>初始条件给出先验分布 $P(\theta)$ 和似然函数 $P(X\mid \theta)$</li>
      <li>观测数据 $X=x$</li>
      <li>将先验更新为后验 $P(\theta\mid X=x)$</li>
    </ul>
  </li>
  <li>获得后验的基本工具（通用的概率工具，并不局限于贝叶斯统计或者机器学习）
    <ul>
      <li>
        <p>贝叶斯规则：逆转条件的顺序</p>

        <p>$P(\theta\mid X=x)=\dfrac{P(X=x\mid \theta)P(\theta)}{P(X=x)}$</p>
      </li>
      <li>
        <p>边缘化：消除不必要的变量</p>

        <p>$P(X=x)=\sum_t P(X=x,\theta=t)$ （这被称为证据）</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="例子">例子</h4>
<ul>
  <li>我们对 $X\mid\theta$ 建模为 $N(\theta,1)$，先验为 $N(0,1)$</li>
  <li>
    <p>假设我们观测到 $X=1$，然后更新先验<br /></p>

\[\begin{aligned}
P(\theta|X=1) &amp;= \dfrac{P(X=1| \theta)P(\theta)}{P(X=1)} \quad\quad\color{purple}{\text{目标是将后验转换为已知分布形式。指数的二次方一定为正态}}\\
&amp;\propto P(X=1| \theta)P(\theta) \\
&amp;=\left[\color{purple}{\dfrac{1}{\sqrt{2\pi}}}\exp\left(-\dfrac{(1-\theta)^2}{2}\right)\right]\left[\color{purple}{\dfrac{1}{\sqrt{2\pi}}}\exp\left(-\frac{\theta^2}{2}\right)\right] \quad\quad\color{purple}{\text{丢弃关于 }\theta\text{ 的常数项}}\\
&amp;\propto \exp\left(-\dfrac{(1-\theta)^2+\theta^2}{2}\right) \quad\quad\color{purple}{\text{合并指数项}}\\
&amp;= \exp\left(-\dfrac{2\theta^2-2\theta+1}{2}\right) \\
&amp;= \exp\left(-\dfrac{\theta^2-\theta+\frac{1}{2}}{2\times \color{purple}{\frac{1}{2}}}\right) \quad\quad\color{purple}{\text{将分子项中 }\theta^2\text{ 的系数移到分母上}}\\
&amp;= \exp\left(-\dfrac{\theta^2-\theta+\color{purple}{\frac{1}{4}}}{2\times \frac{1}{2}}\right) \cdot \color{purple}{\exp\left(-\dfrac{\frac{1}{4}}{2\times \frac{1}{2}}\right)}  \quad\quad\color{purple}{\text{将分子项凑成平方形式：移除多余的常数项}}\\
&amp;\propto \exp\left(-\dfrac{\theta^2-\theta+\frac{1}{4}}{2\times \frac{1}{2}}\right)\\
&amp;= \exp\left(-\dfrac{(\theta-\frac{1}{2})^2}{2\times \frac{1}{2}}\right)  \quad\quad\color{purple}{\text{因式分解}}\\
&amp;\propto N(0.5,0.5) \quad\quad\quad\color{purple}{\text{发现为（非标准）正态分布}}
\end{aligned}\]

    <p>注意：允许将常量提到前面，并通过归一化 “忽略”</p>
  </li>
</ul>

<h3 id="33-如何利用贝叶斯进行点估计">3.3 如何利用贝叶斯进行点估计</h3>
<ul>
  <li>通常我们不这么做，除非被拿枪顶着脑袋。。。
    <ul>
      <li>因为后验已经包含了全部信息，为什么要丢弃掉呢？</li>
    </ul>
  </li>
  <li>但是，还是有一些通用的方法
    <ul>
      <li>后验均值： $E_{\theta\mid X}[\theta]=\int \theta P(\theta\mid X)d\theta$</li>
      <li>后验模式： $\mathop{\operatorname{arg\,max}}\limits_\theta P(\theta\mid X)$ (最大化后验概率 MAP)</li>
      <li>这些是贝叶斯决策理论的解释 
<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8cvty5gc4j30li0b4my8.jpg" width="50%" /></li>
    </ul>
  </li>
</ul>

<h3 id="34-贝叶斯上下文中的-mle">3.4 贝叶斯上下文中的 MLE</h3>
<ul>
  <li>MLE 公式：寻找对数据拟合最好的参数
$\hat\theta\in\mathop{\operatorname{arg\,max}}\limits_\theta P(X=x\mid \theta)$</li>
  <li>
    <p>在贝叶斯公式下，考虑 MAP<br /></p>

\[\begin{aligned}
\hat\theta &amp;\in \mathop{\operatorname{arg\,max}}\limits_\theta P(\theta\mid X=x)\\
           &amp;= \mathop{\operatorname{arg\,max}}\limits_\theta \dfrac{P(X=x\mid \theta)P(\theta)}{P(X=x)} \\
           &amp;= \mathop{\operatorname{arg\,max}}\limits_\theta P(X=x\mid \theta)P(\theta)
\end{aligned}\]
  </li>
  <li>可以看到，和 MLE 相比，MAP 多了一项权重，即先验 $P(\theta)$</li>
  <li>MLE 可以看作 MAP 先验是均匀分布的情况，即 $P(\theta) \propto 1$</li>
</ul>

<h3 id="35-频率学派-vs-贝叶斯学派">3.5 频率学派 vs. 贝叶斯学派</h3>
<ul>
  <li>统计思想的两个主要流派
    <ul>
      <li>决策理论对两者都有补充</li>
    </ul>
  </li>
  <li>过去：两者存在争议甚至敌意; 几乎像是对于 “宗教” 的选择</li>
  <li>如今：两者紧密联系</li>
</ul>

<h2 id="4-概率模型的类别">4. 概率模型的类别</h2>
<h3 id="41-参数模型-vs-非参数模型">4.1 参数模型 vs. 非参数模型</h3>

<table>
  <thead>
    <tr>
      <th>参数模型</th>
      <th>非参数模型</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>由固定的、有限数量的参数确定</td>
      <td>参数数量随着数据的增长而增加，可能有无限多</td>
    </tr>
    <tr>
      <td>缺乏灵活性</td>
      <td>更加灵活</td>
    </tr>
    <tr>
      <td>统计和计算方面更高效</td>
      <td>效率较低</td>
    </tr>
    <tr>
      <td>例如：Logistic 回归、感知机、朴素贝叶斯、简单的神经网络等</td>
      <td>例如：KNN、决策树、SVM 等</td>
    </tr>
  </tbody>
</table>

<p>频率学派和贝叶斯学派都有参数/非参数模型。</p>

<h3 id="42-生成式模型-vs-判别式模型">4.2 生成式模型 vs. 判别式模型</h3>
<ul>
  <li>$X$ 是实例，$Y$ 是标签 （监督学习设置）
    <ul>
      <li>给定：来自独立同分布的数据 $(X_1,Y_1),…,(X_n,Y_n)$</li>
      <li>发现模型，可以在新的 $X$ 上预测 $Y$</li>
    </ul>
  </li>
  <li>生成式方法
    <ul>
      <li>对完整的联合概率建模 $P(X,Y)$</li>
    </ul>
  </li>
  <li>判别式方法
    <ul>
      <li>仅对条件概率建模 $P(Y\mid X)$</li>
    </ul>
  </li>
  <li>两者各有优劣</li>
</ul>

<p>频率学派和贝叶斯学派都有生成式/判别式模型。</p>

<h2 id="总结">总结</h2>
<ul>
  <li>哲学角度：频率学派 vs. 贝叶斯学派</li>
  <li>许多机器学习模型背后的原理：
    <ul>
      <li>MLE</li>
      <li>风险最小化</li>
      <li>概率推断、MAP</li>
    </ul>
  </li>
  <li>参数模型 vs. 非参数模型</li>
  <li>生成式模型 vs. 判别式模型</li>
</ul>

<p>下节内容：线性回归与优化（需要 MLE，ERM 等）</p>

:ET