I"l<h1 id="lecture-12-nn-网络层卷积层">Lecture 12 nn 网络层：卷积层</h1>

<p>在上节课中，我们学习了如何在 PyTorch 中搭建神经网络模型，以及在搭建网络的过程中常用的容器： <code class="language-plaintext highlighter-rouge">Sequential</code>、<code class="language-plaintext highlighter-rouge">ModuleList</code> 和 <code class="language-plaintext highlighter-rouge">ModuleDict</code>。本节课开始，我们将学习 PyTorch 中常见的网络层，现在我们先重点学习卷积层。</p>

<h2 id="1-一维二维和三维卷积">1. 一维、二维和三维卷积</h2>

<p><strong>卷积运算 (Convolution)</strong>：卷积核在输入信号 (图像) 上滑动，相应位置上进行 <strong>乘加</strong>。
<strong>卷积核 (Kernel)</strong>：又称为滤波器/过滤器，可认为是某种模式/某种特征。</p>

<p>卷积过程类似于用一个模版去图像上寻找与它相似的区域，与卷积核模式越相似，激活值越高，从而实现特征提取。所以在深度学习中，我们可以将卷积核视为特征提取器。</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-2D_Convolution_Animation.gif" width="60%" /></p>

<p>下图是 AlexNet 卷积核的可视化，我们发现卷积核实际上学习到的是 <strong>边缘</strong>、<strong>条纹</strong>、<strong>色彩</strong> 这些细节模式：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-WX20201216-103333%402x.png" width="60%" /></p>

<p>这进一步验证了卷积核是图像的某种特征提取器，而具体的特征模式则完全由模型学习得到。</p>

<p><strong>卷积维度 (Dimension)</strong>：<strong>一般情况下</strong>，一个卷积核在一个信号上沿几个维度上滑动，就是几维卷积。</p>

<p><strong>1d 卷积</strong>：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-1d-2.gif" width="60%" /></p>

<p><strong>2d 卷积</strong>：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-2d.gif" width="60%" /></p>

<p><strong>3d 卷积</strong>：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-3d.gif" width="60%" /></p>

<p>可以看到，一个卷积核在一个信号上沿几个维度滑动，就是几维卷积。注意这里我们强调 <strong>一个卷积核</strong> 和 <strong>一个信号</strong>，因为通常我们会涉及包含多个卷积核和多个信号的卷积操作，这种情况下怎么去判断卷积的维度呢，这里我们可以先思考一下。</p>

<h2 id="2-二维卷积">2. 二维卷积</h2>

<h4 id="nnconv2d"><code class="language-plaintext highlighter-rouge">nn.Conv2d</code></h4>

<p><strong>功能</strong>：对多个二维平面信号进行二维卷积。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="o">=</span><span class="s">'zeros'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">in_channels</code>：输入通道数。</li>
  <li><code class="language-plaintext highlighter-rouge">out_channels</code>：输出通道数，等价于卷积核个数。</li>
  <li><code class="language-plaintext highlighter-rouge">kernel_size</code>：卷积核尺寸。</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">stride</code>：步长。下面是一个步长为 2 的卷积：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-step2.gif" width="30%" /></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">padding</code>：填充个数。常用于保持输入输出图像尺寸匹配，可以用于提高输出图像的分辨率：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-padd.gif" width="30%" /></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">dilation</code>：空洞卷积大小。常用于图像分割任务，目的是提高感受野，即输出图像的一个像素对应输入图像上更大的一块区域：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-dila.gif" width="30%" /></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">groups</code>：分组卷积的组数。常用于模型的轻量化。例如，Alexnet 当时由于硬件限制采用了两组卷积操作：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-WX20201216-142158%402x.png" width="80%" /></p>
  </li>
  <li><code class="language-plaintext highlighter-rouge">bias</code>：偏置。最终输出响应值时需加上偏置项。</li>
</ul>

<p><strong>尺寸计算</strong>：</p>

<ul>
  <li>
    <p>简化版 (不带 <code class="language-plaintext highlighter-rouge">padding</code> 和 <code class="language-plaintext highlighter-rouge">dilation</code>)：</p>

\[\mathrm{out}_{\text{size}} = \dfrac{\mathrm{in}_{\text{size}} - \mathrm{kernel}_{\text{size}}}{\mathrm{stride}} + 1\]
  </li>
  <li>
    <p>完整版：</p>

\[H_{\text{out}} = \left\lfloor \dfrac{H_{\text{in}} + 2\times \mathrm{padding}[0] - \mathrm{dilation}[0] \times (\text{kernel_size}[0]-1)-1}{\mathrm{stride}[0]} + 1 \right\rfloor\]
  </li>
</ul>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tools.common_tools</span> <span class="kn">import</span> <span class="n">transform_invert</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># 设置随机种子，用于调整卷积核权值的状态。
</span>
<span class="c1"># ================================= load img ==================================
</span><span class="n">path_img</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">__file__</span><span class="p">)),</span> <span class="s">"lena.png"</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">path_img</span><span class="p">).</span><span class="n">convert</span><span class="p">(</span><span class="s">'RGB'</span><span class="p">)</span>  <span class="c1"># 0~255
</span>
<span class="c1"># convert to tensor
</span><span class="n">img_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">()])</span>
<span class="n">img_tensor</span> <span class="o">=</span> <span class="n">img_transform</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">img_tensor</span><span class="p">.</span><span class="n">unsqueeze_</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>    <span class="c1"># C*H*W to B*C*H*W
</span>
<span class="c1"># ========================= create convolution layer ==========================
</span><span class="n">conv_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>   <span class="c1"># input:(i, o, size) weights:(o, i , h, w)
</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">conv_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># calculation
</span><span class="n">img_conv</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>

<span class="c1"># =========================== visualization ==================================
</span><span class="k">print</span><span class="p">(</span><span class="s">"卷积前尺寸:{}</span><span class="se">\n</span><span class="s">卷积后尺寸:{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">img_conv</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">img_conv</span> <span class="o">=</span> <span class="n">transform_invert</span><span class="p">(</span><span class="n">img_conv</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">...],</span> <span class="n">img_transform</span><span class="p">)</span>
<span class="n">img_raw</span> <span class="o">=</span> <span class="n">transform_invert</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">img_transform</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">).</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_conv</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">).</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_raw</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>卷积前尺寸:torch.Size([1, 3, 512, 512])
卷积后尺寸:torch.Size([1, 1, 510, 510])
</pre></td></tr></tbody></table></code></pre></div></div>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">set.seed(1)</code> 时的输出：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-WX20201216-150359%402x.png" width="70%" /></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">set.seed(2)</code> 时的输出：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-WX20201216-150625%402x.png" width="70%" /></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">set.seed(3)</code> 时的输出：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-WX20201216-145128%402x.png" width="70%" /></p>
  </li>
</ul>

<p>可以看到，不同的卷积核权值对应的输出是不相同的。通常，我们会在卷积层中设置多个卷积核，以提取不同的特征。</p>

<p>在上面的例子中，我们使用一个 3 维的卷积核实现了一个 2d 卷积：</p>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-cnn_1.gif" /></p>

<p>我们的输入是一个 RGB 的二维图像，它包含 3 个色彩通道。然后，我们将创建 3 个二维卷积核，不同通道对应不同的卷积核。我们将三个通道的卷积结果相加，然后再加上偏置项，得到最终的卷积结果。</p>

<h2 id="3-转置卷积">3. 转置卷积</h2>

<p><strong>转置卷积 (Transpose Convolution)</strong> 又称为 <strong>反卷积 (Deconvolution)</strong><sup>注 1</sup> 或者 <strong>部分跨越卷积 (Fractionally strided Convolution)</strong>，常见于图像分割任务中，主要用于对图像进行 <strong>上采样 (UpSample)</strong>。</p>

<p>(注 1：这里我们说的反卷积不同于信号系统中的反卷积)。</p>

<p><strong>为什么称为转置卷积？</strong></p>

<ul>
  <li>
    <p><strong>正常卷积</strong>：</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-conv.gif" width="30%" /></p>

    <p>假设图像尺寸为 $4 \times 4$，卷积核为 $3\times 3$，$\mathrm{padding} = 0$，$\mathrm{stride} = 1$。</p>

    <ul>
      <li><strong>图像</strong>：$I_{16\times 1}$，这里 $16$ 是输入图像的像素总数，$1$ 表示图片张数。</li>
      <li><strong>卷积核</strong>：$K_{4\times 16}$，这里 $4$ 是输出图像的像素总数，$16$ 是由卷积核中的 $9$ 个元素另外补零后得到。</li>
      <li><strong>输出</strong>：$O_{4\times 1} = K_{\color{orange}{4\times 16}} \times I_{16\times 1}$</li>
    </ul>

    <p><br /></p>
  </li>
  <li>
    <p><strong>转置卷积</strong>：上采样，输出图像比输入图像尺寸更大。</p>

    <p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-t.gif" width="30%" /></p>

    <p>假设图像尺寸为 $2 \times 2$，卷积核为 $3\times 3$，$\mathrm{padding} = 0$，$\mathrm{stride} = 1$。</p>

    <ul>
      <li><strong>图像</strong>：$I_{4\times 1}$，这里 $4$ 是输入图像的像素总数，$1$ 表示图片张数。</li>
      <li><strong>卷积核</strong>：$K_{16\times 4}$，这里 $16$ 是输出图像的像素总数，$4$ 是由卷积核中的 $9$ 个元素剔除一部分后得到。</li>
      <li><strong>输出</strong>：$O_{16\times 1} = K_{\color{orange}{16\times 4}} \times I_{4\times 1}$</li>
    </ul>
  </li>
</ul>

<p>可以看到，转置卷积与正常卷积的卷积核尺寸在形状上是转置关系，这也是我们将其称为转置卷积的原因。注意，二者只是在形状上是转置关系，但它们的权值是完全不同的。也就是说，该卷积过程是不可逆的，即卷积后再转置卷积，得到的图像和初始图像是完全不同的。</p>

<h4 id="nnconvtranspose2d"><code class="language-plaintext highlighter-rouge">nn.ConvTranspose2d</code></h4>

<p><strong>功能</strong>：转置卷积实现上采样。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="n">nn</span><span class="p">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="o">=</span><span class="s">'zeros'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>主要参数</strong>：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">in_channels</code>：输入通道数。</li>
  <li><code class="language-plaintext highlighter-rouge">out_channels</code>：输出通道数。</li>
  <li><code class="language-plaintext highlighter-rouge">kernel_size</code>：卷积核尺寸。</li>
  <li><code class="language-plaintext highlighter-rouge">stride</code>：步长。</li>
  <li><code class="language-plaintext highlighter-rouge">padding</code>：填充个数。</li>
  <li><code class="language-plaintext highlighter-rouge">dilation</code>：空洞卷积大小。</li>
  <li><code class="language-plaintext highlighter-rouge">groups</code>：分组卷积设置。</li>
  <li><code class="language-plaintext highlighter-rouge">bias</code>：偏置。</li>
</ul>

<p><strong>尺寸计算</strong>：</p>

<ul>
  <li>
    <p>简化版 (不带 <code class="language-plaintext highlighter-rouge">padding</code> 和 <code class="language-plaintext highlighter-rouge">dilation</code>)：</p>

\[\mathrm{out}_{\text{size}} = (\mathrm{in}_{\text{size}} -1)\times \mathrm{stride} + \mathrm{kernel}_{\text{size}}\]
  </li>
  <li>
    <p>完整版：</p>

\[H_{\text{out}} = (H_{\text{in}}-1) \times \mathrm{stride}[0] - 2 \times \mathrm{padding}[0] + \mathrm{dilation}[0] \times (\text{kernel_size}[0]-1) + \mathrm{output\_padding}[0]+ 1\]
  </li>
</ul>

<p><strong>代码示例</strong>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tools.common_tools</span> <span class="kn">import</span> <span class="n">transform_invert</span><span class="p">,</span> <span class="n">set_seed</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># 设置随机种子，用于调整卷积核权值的状态。
</span>
<span class="c1"># ================================= load img ==================================
</span><span class="n">path_img</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">__file__</span><span class="p">)),</span> <span class="s">"lena.png"</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">path_img</span><span class="p">).</span><span class="n">convert</span><span class="p">(</span><span class="s">'RGB'</span><span class="p">)</span>  <span class="c1"># 0~255
</span>
<span class="c1"># convert to tensor
</span><span class="n">img_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">()])</span>
<span class="n">img_tensor</span> <span class="o">=</span> <span class="n">img_transform</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">img_tensor</span><span class="p">.</span><span class="n">unsqueeze_</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>    <span class="c1"># C*H*W to B*C*H*W
</span>
<span class="c1"># ========================= create convolution layer ==========================
</span><span class="n">conv_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>   <span class="c1"># input:(i, o, size)
</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">conv_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># calculation
</span><span class="n">img_conv</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">)</span>

<span class="c1"># =========================== visualization ==================================
</span><span class="k">print</span><span class="p">(</span><span class="s">"卷积前尺寸:{}</span><span class="se">\n</span><span class="s">卷积后尺寸:{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">img_conv</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">img_conv</span> <span class="o">=</span> <span class="n">transform_invert</span><span class="p">(</span><span class="n">img_conv</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">...],</span> <span class="n">img_transform</span><span class="p">)</span>
<span class="n">img_raw</span> <span class="o">=</span> <span class="n">transform_invert</span><span class="p">(</span><span class="n">img_tensor</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">img_transform</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">).</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_conv</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">).</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_raw</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>输出结果：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>卷积前尺寸:torch.Size([1, 3, 512, 512])
卷积后尺寸:torch.Size([1, 1, 1025, 1025])
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="http://andy-blog.oss-cn-beijing.aliyuncs.com/blog/2020-12-16-WX20201216-161631%402x.png" width="70%" /></p>

<p>可以看到，在经过转置卷积上采样后，图像出现了一个奇怪的现象：输出的图像上有许多网格。这被称为 <strong>棋盘效应 (Checkerboard Artifacts)</strong>，是由于转置卷积中的不均匀重叠造成的。关于棋盘效应的解释以及解决方法请参考论文 <em><a href="https://www.researchgate.net/publication/312511634_Deconvolution_and_Checkerboard_Artifacts">Deconvolution and Checkerboard Artifacts</a></em>。</p>

<h2 id="4-总结">4. 总结</h2>

<p>本节课中，我们学习了 <code class="language-plaintext highlighter-rouge">nn</code> 模块中卷积层。在下次课程中，我们将学习 <code class="language-plaintext highlighter-rouge">nn</code> 模块中的其他常用网络层。</p>

<p>下节内容：nn 网络层：池化层、线性层和激活函数层</p>
:ET